{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Installation of OpenAI API and langchain\n",
        "#@markdown Note: Make sure to run all code cells (play button next to code cells or Runtime->Run all)\n",
        "%pip install langchain==0.0.140\n",
        "%pip install openai"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SZ21UNlKwGI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Insert API key and scenario\n",
        "\n",
        "openai_api_key = 'sk-abcdef123... !Only GPT-4 access-enabled API keys are supported!' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Instructions: \n",
        "#@markdown - You'll need an OpenAI API key with access to GPT-4!\n",
        "#@markdown - Provide a starting point for the simulation. Optional: Can include/imply a perturbation\n",
        "#@markdown   - e.g., a situation or experimental setup or a detailed/complex question that will be answered using a simulation.\n",
        "#@markdown   - for more examples, check out the `prompts/` folders [in our repository](https://github.com/OpenBioLink/SimulateGPT/tree/main/experiments)\n",
        "#@markdown - If you expect a final outcome, explicitly request it (use the words) ‘final outcome’\n",
        "#@markdown - Optional: You can increase the novelty by adding: \"Focus on more novelty.\"\n",
        "#@markdown - SimulateGPT can be used to ask detailed/complex questions about biology. \n",
        "#@markdown - SimulateGPT has the potential to assess the question in more depth and provide more informed answers than the default ChatGPT.\n",
        "#@markdown - For chat-like follow-up questions, refer to the OpenAI playground and provide the SimulateGPT prompt as SYSTEM message\n",
        "\n",
        "simulation_scenario = 'A wild type mouse is injected with YUMM 1.7. Report the most relevant final outcome.' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Use the default SimulateGPT prompt or one of variations, described in the paper.\n",
        "\n",
        "complexity_input = \"SimulateGPT (default)\" #@param [\"SimulateGPT (default)\", \"Low-complexity SimulateGPT\", \"5-step SimulateGPT\"]\n",
        "\n",
        "# translate to prompt filename\n",
        "complexity_fn = dict(zip(\n",
        "    [\"SimulateGPT (default)\", \"Low-complexity SimulateGPT\", \"5-step SimulateGPT\"], \n",
        "    [\"high_complexity\", \"low_complexity\", \"high5step_complexity\"]))[complexity_input]\n"
      ],
      "metadata": {
        "id": "W1mBbYo5wX_E",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load OpenAI API and SimulateGPT\n",
        "\n",
        "import os \n",
        "import urllib.request\n",
        "from langchain.callbacks.base import CallbackManager\n",
        "\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "def load_openai_api(api_key: str, model=\"gpt-4\"):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        api_key: The OpenAI API key to use. If None, then try to load it from the environment variable OPENAI_API_KEY or from the password store.\n",
        "        model: The model to use. Defaults to \"gpt-4\". alternatively use gpt-3.5-turbo for faster and cheaper exploration\n",
        "    \"\"\"\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    stream_chat = ChatOpenAI(\n",
        "        streaming=True,\n",
        "        temperature=0,\n",
        "        model_name=model,\n",
        "        request_timeout=60,\n",
        "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
        "        verbose=True,\n",
        "    )\n",
        "    return stream_chat\n",
        "stream_chat = load_openai_api(openai_api_key)\n",
        "\n",
        "# Load the system prompt from GitHub\n",
        "simulate_gpt_url = f\"https://raw.githubusercontent.com/OpenBioLink/SimulateGPT/main/system_messages/{complexity_fn}\"\n",
        "\n",
        "response = urllib.request.urlopen(simulate_gpt_url)\n",
        "data = response.read()\n",
        "simulate_gpt_system = data.decode(\"utf-8\")\n",
        "print(\"Loaded API and SimulateGPT\")"
      ],
      "metadata": {
        "id": "51pr2OL-yd9S",
        "cellView": "form",
        "outputId": "baefc67a-b219-440c-d3a0-cb6e6a79cb73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded API and SimulateGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run SimulateGPT through OpenAI GPT-4 API\n",
        "\n",
        "request = [\n",
        "    SystemMessage(content=simulate_gpt_system),\n",
        "    HumanMessage(content=simulation_scenario),\n",
        "]\n",
        "result = stream_chat(request)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "6MAkdYkkyPZN",
        "outputId": "ff45445c-ec72-4090-d923-432826f0a91f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters:\n",
            "  - wild type mouse\n",
            "  - YUMM 1.7 injection\n",
            "\n",
            "simulation:\n",
            "  - step: 1\n",
            "    level: cellular\n",
            "    facts: YUMM 1.7 is a murine melanoma cell line derived from a BRAFV600E-driven melanoma in a C57BL/6 mouse [1]. Melanoma is a type of skin cancer that arises from melanocytes, the pigment-producing cells [2]. The BRAFV600E mutation is a common driver of melanoma, leading to constitutive activation of the MAPK/ERK signaling pathway, promoting cell proliferation and survival [3].\n",
            "    entities: YUMM 1.7, melanoma, melanocytes, BRAFV600E, MAPK/ERK signaling pathway\n",
            "    assumptions: The injected YUMM 1.7 cells are viable and capable of proliferating in the wild type mouse.\n",
            "    consequence: engraftment of YUMM 1.7 cells in the mouse\n",
            "    probability: 80\n",
            "    explanation: The YUMM 1.7 cells are derived from a C57BL/6 mouse, which is the same genetic background as the wild type mouse. This increases the likelihood of successful engraftment and growth of the melanoma cells in the mouse.\n",
            "    novelty: 0\n",
            "\n",
            "  - step: 2\n",
            "    level: tissue\n",
            "    facts: Melanoma cells can invade surrounding tissues and metastasize to distant organs, such as the lungs, liver, and brain [4]. The process of metastasis involves local invasion, intravasation into blood or lymphatic vessels, survival in circulation, extravasation, and colonization at a distant site [5].\n",
            "    entities: local invasion, intravasation, circulation, extravasation, colonization, metastasis\n",
            "    assumptions: The engrafted YUMM 1.7 cells are capable of invading surrounding tissues and metastasizing to distant organs in the wild type mouse.\n",
            "    consequence: development of metastatic melanoma in the mouse\n",
            "    probability: 70\n",
            "    explanation: The YUMM 1.7 cells harbor the BRAFV600E mutation, which is known to drive melanoma progression and metastasis. The injected cells are likely to invade surrounding tissues and metastasize to distant organs in the mouse.\n",
            "    novelty: 0\n",
            "\n",
            "conclusion:\n",
            "  outcome: development of metastatic melanoma in the wild type mouse\n",
            "  explanation: The injection of YUMM 1.7 cells, which are derived from a BRAFV600E-driven melanoma, is likely to result in the engraftment and growth of melanoma cells in the wild type mouse. These cells have the potential to invade surrounding tissues and metastasize to distant organs, leading to the development of metastatic melanoma in the mouse.\n",
            "\n",
            "references:\n",
            "  \"[1]\": \"Meeth K, Wang JX, Micevic G, Damsky W, Bosenberg MW. 2016. The YUMM lines: a series of congenic mouse melanoma cell lines with defined genetic alterations. Pigment Cell Melanoma Res. 29(5):590-7.\"\n",
            "  \"[2]\": \"Schadendorf D, van Akkooi ACJ, Berking C, Griewank KG, Gutzmer R, Hauschild A, Stang A, Roesch A, Ugurel S. 2018. Melanoma. Lancet. 392(10151):971-984.\"\n",
            "  \"[3]\": \"Davies H, Bignell GR, Cox C, et al. 2002. Mutations of the BRAF gene in human cancer. Nature. 417(6892):949-54.\"\n",
            "  \"[4]\": \"Luke JJ, Flaherty KT, Ribas A, Long GV. 2017. Targeted agents and immunotherapies: optimizing outcomes in melanoma. Nat Rev Clin Oncol. 14(8):463-482.\"\n",
            "  \"[5]\": \"Lambert AW, Pattabiraman DR, Weinberg RA. 2017. Emerging Biological Principles of Metastasis. Cell. 168(4):670-691.\""
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "a45f68e904890b8c321d3b9208e7b6a602b0eb7b894f25513a607ffd68e58ab2"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
